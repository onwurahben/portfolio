<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice AI Assistant ‚Äî Ben Onwurah</title>
    <link rel="stylesheet" href="../css/style.css">
    <link rel="stylesheet" href="../css/project.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Fraunces:ital,opsz,wght@0,9..144,100..900;1,9..144,100..900&family=Outfit:wght@100..900&display=swap"
        rel="stylesheet">
</head>

<body class="project-detail-body">

    <a href="../index.html" class="back-home">
        <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
            stroke-linecap="round" stroke-linejoin="round">
            <line x1="19" y1="12" x2="5" y2="12"></line>
            <polyline points="12 19 5 12 12 5"></polyline>
        </svg>
        Back to homepage
    </a>

    <header class="project-hero reveal">
        <span class="section-id">MULTIMODAL_AI / 01</span>
        <h1>AI Voice Assistant</h1>

        <div class="project-meta-grid">
            <div class="meta-item">
                <h5>Role</h5>
                <p>AI Engineer</p>
            </div>
            <div class="meta-item">
                <h5>Focus</h5>
                <p>Real-time Audio & TTS</p>
            </div>
            <div class="meta-item">
                <h5>Stack</h5>
                <p>Whisper, LLaMA 3, Google TTS</p>
            </div>
        </div>
    </header>

    <section class="project-showcase">
        <video autoplay loop muted playsinline shadow>
            <source src="../images/voice-demo.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>
    </section>

    <main class="project-content-grid">
        <div class="main-content reveal">
            <p class="intro">
                I designed and deployed a production-ready voice AI assistant that enables natural, real-time
                conversations
                by orchestrating speech detection, transcription, reasoning, and speech synthesis in a single
                low-latency pipeline.
            </p>

            <p class="meta">
                ‚ñ∂ <a href="#" target="_blank">Live Demo: Hugging Face Space</a><br>
                üéôÔ∏è <strong>Tech Stack:</strong> Python ¬∑ Flask ¬∑ LLaMA 3 (70B) ¬∑ Whisper ¬∑ Groq API ¬∑ ElevenLabs ¬∑
                Google TTS ¬∑ Docker
            </p>
        </div>

        <aside class="detail-sidebar reveal delay-1">
            <div class="sidebar-item">
                <h6>Optimization</h6>
                <p>Client-side Voice Activity Detection (VAD)</p>
            </div>
            <div class="sidebar-item">
                <h6>Performance</h6>
                <p>Ultra-Fast Inference with Groq</p>
            </div>
        </aside>
    </main>

    <div class="project-body-full">
        <!-- PROBLEM -->
        <section class="reveal">
            <h2>The Problem</h2>
            <p>
                Most voice assistants fail in real-world usage due to latency, poor speech detection, robotic audio
                output,
                or high operational costs. Systems often record silence, misinterpret background noise, or generate
                responses
                that sound unnatural when spoken aloud.
            </p>
            <p>
                For conversational AI to feel usable, it must:
            </p>
            <ul>
                <li>Detect when a user is actually speaking</li>
                <li>Respond quickly enough to feel interactive</li>
                <li>Produce audio that sounds natural, not synthetic</li>
                <li>Maintain conversational context across turns</li>
            </ul>
            <p>
                <strong>The challenge:</strong> build a voice-first AI system that feels responsive, human, and
                production-ready ‚Äî not experimental.
            </p>
        </section>

        <!-- SOLUTION -->
        <section class="reveal">
            <h2>The Solution</h2>
            <p>
                This project is a multimodal AI voice assistant that combines real-time speech detection, transcription,
                LLM reasoning, and high-quality text-to-speech into a single cohesive system.
            </p>
            <p>
                The assistant listens intelligently, reasons accurately, and responds with natural-sounding speech ‚Äî
                while minimizing unnecessary API calls and latency.
            </p>
        </section>

        <!-- ARCHITECTURE -->
        <section class="reveal">
            <h2>System Architecture</h2>
            <p>
                The system is designed as a low-latency, modular pipeline:
            </p>
            <ul>
                <li>Client-side Voice Activity Detection (VAD) analyzes audio energy in real time</li>
                <li>Only validated speech is sent for transcription</li>
                <li>Whisper (via Groq) converts speech to text</li>
                <li>LLaMA 3 (70B) generates a contextual response</li>
                <li>Output text is cleaned and optimized for speech</li>
                <li>A TTS router selects the best available voice provider</li>
                <li>Audio is streamed back to the user</li>
                <li>Session-based memory maintains conversational context</li>
            </ul>

            <div class="architecture-diagram">
                <img src="../images/voice-architecture.svg" alt="Voice AI Assistant Architecture Diagram">
            </div>

            <p>
                This architecture ensures both responsiveness and cost efficiency.
            </p>
        </section>

        <!-- ENGINEERING DECISIONS -->
        <section class="reveal">
            <h2>Key Engineering Decisions</h2>

            <h3>Client-Side Voice Activity Detection (VAD)</h3>
            <p>
                Rather than relying on server-side filtering, the system performs RMS-based speech detection on the
                client.
                This prevents silent or noisy audio from being sent to the backend, reducing costs and improving
                responsiveness
                across all languages.
            </p>

            <h3>Ultra-Fast Inference with Groq</h3>
            <p>
                Both Whisper (STT) and LLaMA 3 (LLM) are served via Groq to minimize end-to-end latency, enabling near
                real-time
                conversational flow.
            </p>

            <h3>Multi-Provider TTS Architecture</h3>
            <p>
                The system supports multiple TTS providers with automatic routing:
            </p>
            <ul>
                <li>ElevenLabs for premium, human-like voices</li>
                <li>Google Cloud TTS as a reliable low-cost fallback</li>
            </ul>
            <p>
                This design avoids single-provider lock-in and improves system resilience.
            </p>

            <h3>Session-Based Memory Management</h3>
            <p>
                Each user session maintains its own conversational state, enabling coherent multi-turn conversations
                while
                remaining Docker-compatible and safe for concurrent users.
            </p>
        </section>

        <!-- CAPABILITIES -->
        <section class="reveal">
            <h2>Core Capabilities</h2>
            <ul>
                <li>Real-time voice interaction with intelligent speech detection</li>
                <li>Multilingual speech-to-text and text-to-speech support</li>
                <li>Natural-sounding audio output with multiple voice profiles</li>
                <li>Persistent conversational memory across turns</li>
                <li>Cost-optimized audio processing pipeline</li>
                <li>Fully containerized deployment for cloud or local environments</li>
            </ul>
        </section>

        <!-- PRODUCTION IMPLEMENTATION -->
        <section class="reveal">
            <h2>Production-Grade Implementation</h2>
            <ul>
                <li><strong>Backend:</strong> Flask-based orchestration layer</li>
                <li><strong>STT:</strong> Groq-hosted Whisper Large v3</li>
                <li><strong>LLM:</strong> LLaMA 3 (70B) via Groq</li>
                <li><strong>TTS:</strong> ElevenLabs + Google Cloud Text-to-Speech</li>
                <li><strong>Frontend:</strong> Lightweight web UI with client-side audio processing</li>
                <li><strong>Deployment:</strong> Docker & Docker Compose</li>
            </ul>
            <p>
                The system was engineered with stability, latency, and extensibility in mind.
            </p>
        </section>

        <!-- PERFORMANCE -->
        <section class="reveal">
            <h2>Performance Characteristics</h2>
            <ul>
                <li><strong>Speech-to-Text:</strong> ~2‚Äì3 seconds</li>
                <li><strong>LLM Reasoning:</strong> ~1‚Äì2 seconds</li>
                <li><strong>Text-to-Speech:</strong> ~1‚Äì2 seconds (ElevenLabs)</li>
                <li><strong>End-to-End Interaction:</strong> ~5‚Äì8 seconds per conversational turn</li>
            </ul>
            <p>
                These timings enable smooth, natural-feeling dialogue.
            </p>
        </section>

        <!-- OUTCOME -->
        <section class="reveal">
            <h2>Outcome & Impact</h2>
            <ul>
                <li>Built a fully integrated multimodal AI assistant from microphone input to spoken response</li>
                <li>Reduced unnecessary API usage through intelligent audio preprocessing</li>
                <li>Demonstrated production-level handling of latency, cost, and user experience</li>
            </ul>
            <p>
                This project showcases applied AI engineering beyond text ‚Äî integrating audio, language, and system
                design.
            </p>
        </section>

        <!-- DEMO -->
        <section class="reveal">
            <h2>Live Demo</h2>
            <p>
                ‚ñ∂ <a href="#" target="_blank">Try the AI Voice Assistant live on Hugging Face</a><br>
                Interact with the system in real time and experience the full voice pipeline in action.
            </p>
        </section>

        <!-- SKILLS -->
        <section class="reveal">
            <h2>Skills Demonstrated</h2>
            <ul>
                <li>Multimodal AI System Design</li>
                <li>Speech-to-Text & Text-to-Speech Pipelines</li>
                <li>Low-Latency LLM Orchestration</li>
                <li>Client-Side Signal Processing</li>
                <li>Dockerized AI Deployment</li>
                <li>Conversational UX Engineering</li>
            </ul>
        </section>

        <!-- IMPROVEMENTS -->
        <section class="reveal">
            <h2>Planned Improvements</h2>
            <ul>
                <li>Streaming partial responses for faster perceived latency</li>
                <li>Advanced turn-taking and interruption handling</li>
                <li>User authentication and voice personalization</li>
                <li>Expanded multilingual optimization</li>
            </ul>
        </section>

        <!-- CLOSING -->
        <section class="closing reveal">
            <h2>Why This Project Matters</h2>
            <p>
                Voice interfaces succeed or fail on responsiveness and conversational flow.
                This project focuses on reducing latency and preserving context so interactions
                feel natural rather than fragmented or delayed.
            </p>

        </section>

    </div>

    <footer>
        <p>¬© 2026 Ben Onwurah</p>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const observer = new IntersectionObserver((entries) => {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        entry.target.classList.add('active');
                    }
                });
            }, { threshold: 0.1 });

            document.querySelectorAll('.reveal').forEach(el => observer.observe(el));
        });
    </script>
</body>

</html>