<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DocQuery Assistant â€” Ben Onwurah</title>
    <link rel="stylesheet" href="../css/style.css">
    <link rel="stylesheet" href="../css/project.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Fraunces:ital,opsz,wght@0,9..144,100..900;1,9..144,100..900&family=Outfit:wght@100..900&display=swap"
        rel="stylesheet">
</head>

<body class="project-detail-body">

    <a href="../index.html" class="back-home">
        <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
            stroke-linecap="round" stroke-linejoin="round">
            <line x1="19" y1="12" x2="5" y2="12"></line>
            <polyline points="12 19 5 12 12 5"></polyline>
        </svg>
        Back to archive
    </a>

    <header class="project-hero reveal">
        <span class="section-id">SYSTEM_DESIGN / 01</span>
        <h1>DocQuery Assistant</h1>

        <div class="project-meta-grid">
            <div class="meta-item">
                <h5>Role</h5>
                <p>AI Systems Engineer</p>
            </div>
            <div class="meta-item">
                <h5>Focus</h5>
                <p>RAG & Information Retrieval</p>
            </div>
            <div class="meta-item">
                <h5>Stack</h5>
                <p>Python, OpenAI, Pinecone</p>
            </div>
        </div>
    </header>

    <section class="project-showcase">
        <video autoplay loop muted playsinline>
            <source src="../images/rag-demo.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>
    </section>

    <main class="project-content-grid">
        <div class="main-content reveal">
            <p class="intro">
                I designed and deployed a production-ready RAG system that enables accurate,
                low-latency querying of private PDF documents by grounding LLM responses
                in verified document context.
            </p>

            <p class="meta">
                â–¶ <a href="#" target="_blank">Live Demo: Hugging Face Space</a><br>
                ðŸ“„ <strong>Tech Stack:</strong> Python Â· LangChain (LCEL) Â· Pinecone Â· Groq API Â·
                Llama 3 Â· MiniLM Embeddings Â· Gradio
            </p>
        </div>

        <aside class="detail-sidebar reveal delay-1">
            <div class="sidebar-item">
                <h6>Key Metric</h6>
                <p>70% reduction in search time</p>
            </div>
            <div class="sidebar-item">
                <h6>Infrastructure</h6>
                <p>Deployed as a scalable microservice with FastAPI and Docker.</p>
            </div>
        </aside>
    </main>

    <div class="project-body-full">
        <!-- PROBLEM -->
        <section class="reveal">
            <h2>The Problem</h2>

            <p>
                Organizations store critical knowledge in unstructured documents such as PDFs,
                manuals, and reports. Retrieving precise answers from these documents is slow
                and inefficient, while naÃ¯ve LLM usage often leads to hallucinations or
                irrelevant responses.
            </p>

            <p>
                Traditional keyword search fails to capture semantic meaning, and generic
                chatbots lack grounding in source data.
            </p>

            <p>
                <strong>The challenge:</strong> build a system that delivers fast, accurate,
                and context-aware answers â€” grounded strictly in the uploaded documents.
            </p>
        </section>

        <!-- SOLUTION -->
        <section class="reveal">
            <h2>The Solution</h2>

            <p>
                DocQuery is a Retrieval-Augmented Generation (RAG) assistant that combines
                semantic search with LLM reasoning to answer questions directly from private
                documents.
            </p>

            <p>
                The system retrieves the most relevant document chunks using vector similarity
                search and injects them into a structured prompt pipeline, ensuring responses
                are both accurate and explainable.
            </p>
        </section>

        <!-- ARCHITECTURE -->
        <section class="reveal">
            <h2>System Architecture</h2>

            <ul>
                <li>Users upload one or more PDF documents</li>
                <li>Documents are parsed, chunked, and embedded using MiniLM</li>
                <li>Embeddings are stored in a Pinecone vector database</li>
                <li>User queries are embedded and matched via top-K similarity search</li>
                <li>Retrieved context is injected into an LCEL-style prompt</li>
                <li>Llama 3 (served via Groq API) generates grounded responses</li>
                <li>Results are delivered through a clean chat interface</li>
            </ul>

            <p>
                This architecture cleanly separates retrieval, prompt orchestration, and
                generation, making the system extensible and production-ready.
            </p>
        </section>

        <!-- ENGINEERING DECISIONS -->
        <section class="reveal">
            <h2>Key Engineering Decisions</h2>

            <h3>Vector Database: Pinecone</h3>
            <p>
                Chosen for managed scalability, low-latency similarity search, and reliability
                in production semantic retrieval workloads.
            </p>

            <h3>Embeddings: all-MiniLM-L6-v2</h3>
            <p>
                Selected to balance embedding quality, speed, and cost â€” ideal for real-time
                document querying.
            </p>

            <h3>LLM Inference: Llama 3 via Groq</h3>
            <p>
                Groqâ€™s ultra-fast inference significantly reduces latency, enabling smooth,
                interactive user experiences rather than batch-style responses.
            </p>

            <h3>Prompt Orchestration: LCEL-Style Chaining</h3>
            <p>
                Structured message composition ensures clear separation between system
                instructions, retrieved context, and user input â€” reducing hallucinations
                and improving consistency.
            </p>
        </section>

        <!-- CAPABILITIES -->
        <section class="reveal">
            <h2>Core Capabilities</h2>

            <ul>
                <li>Semantic retrieval across multiple PDF documents</li>
                <li>Top-K chunk selection for highly relevant context</li>
                <li>Persistent conversational memory across sessions</li>
                <li>Retry mechanism to explore alternative grounded responses</li>
                <li>Clean, professional chat interface optimized for real usage</li>
            </ul>
        </section>

        <!-- OUTCOME -->
        <section class="reveal">
            <h2>Outcome & Impact</h2>

            <ul>
                <li>Delivered a complete end-to-end RAG pipeline from ingestion to inference</li>
                <li>Achieved fast response times suitable for interactive knowledge access</li>
                <li>
                    Built a reusable architecture applicable to internal knowledge bases,
                    research assistants, and support tools
                </li>
            </ul>

            <p>
                This project demonstrates applied AI engineering beyond experimentation â€”
                with real deployment constraints in mind.
            </p>
        </section>

        <!-- DEMO -->
        <section class="reveal">
            <h2>Live Demo</h2>

            <p>
                â–¶ <a href="#" target="_blank">Try the system live on Hugging Face</a><br>
                An interactive demo running in a production-style environment.
            </p>
        </section>

        <!-- SKILLS -->
        <section class="reveal">
            <h2>Skills Demonstrated</h2>

            <ul>
                <li>Retrieval-Augmented Generation (RAG)</li>
                <li>Vector Databases & Semantic Search</li>
                <li>LLM Prompt Orchestration (LCEL)</li>
                <li>Cloud-Based AI Deployment</li>
                <li>System Design & Trade-Off Analysis</li>
            </ul>
        </section>

        <!-- IMPROVEMENTS -->
        <section class="reveal">
            <h2>Planned Improvements</h2>

            <ul>
                <li>Document-level metadata filtering</li>
                <li>Hybrid semantic + keyword retrieval</li>
                <li>Retrieval quality evaluation metrics</li>
                <li>User authentication and document isolation</li>
            </ul>
        </section>

        <!-- CLOSING -->
        <section class="closing reveal">
            <h2>Why This Project Matters</h2>

            <p>
                This project reflects how I approach AI engineering:
                <strong>not just building demos, but designing systems that can realistically ship.</strong>
            </p>
        </section>
    </div>

    <footer>
        <p>Â© 2026 Ben Onwurah</p>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const observer = new IntersectionObserver((entries) => {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        entry.target.classList.add('active');
                    }
                });
            }, { threshold: 0.1 });

            document.querySelectorAll('.reveal').forEach(el => observer.observe(el));
        });
    </script>
</body>

</html>